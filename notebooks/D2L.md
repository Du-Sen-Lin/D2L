# D2L

D2L学习记录

doc:

book: https://zh-v2.d2l.ai/chapter_introduction/index.html

gh: https://github.com/d2l-ai/d2l-zh



## 一、前言

presentation: 通俗的介绍了机器学习的发展，机器学习的分支和发展方向，以及一些术语和相关常用概念的通俗理解（比如数据集、机器学习的大概流程等）。



## 二、预备知识

### 2-1、数据操作

presentation: dl用于存储和操作数据的主要接口：张量（n维数组），提供了一些基本功能：包括基本数学运算、广播、索引、切片、内存节省和转换其他 Python 对象。

### 2-2、数据预处理

presentation：pandas的使用，csv表读写和处理缺失数据。

### 2-3、线性代数

presebtation: 线性代数中基本知识，标量、向量、矩阵、张量以及相关操作，范数知识。

### 2-4、微分

presentation： 微分

### 2-5、自动求导

presentation:深度学习中的自动求导。

### 2-6、期望

presentation: 概率分布、联合概率；条件概率；贝叶斯定理；边际化；
独立性；期望和方差。

```
方差： Var[X]=E[(X−E[X])2]=E[X2]−E[X]2

标准差： 方差的平方根

随机变量函数的方差衡量的是，当从该随机变量分布中采样不同值  x  时，函数值偏离该函数的期望的程度：
```

### 2-7、查阅文档

#### 1、查找模块中的所有函数和类
调用dir函数:知道模块中可以调用哪些函数和类;例如，我们可以查询随机数生成模块中的所有属性：

```python
import torch

print(dir(torch.distributions))
```

通常，我们可以忽略以 __ 开始和结束的函数（Python 中的特殊对象）或以单个 _ 开始的函数（通常是内部函数）。

根据剩余的函数名或属性名，我们可能会猜测这个模块提供了各种生成随机数的方法，

包括从均匀分布（uniform）、正态分布 （normal）和多项分布（multinomial）中采样。

#### 2、查找特定函数和类的用法

调用 `help` 函数

```python
help(torch.ones)
```

在Jupyter记事本中，我们可以使用 `?` 在另一个窗口中显示文档。例如，`list?`将创建与`help(list)` 几乎相同的内容。

此外，如果我们使用两个问号，如 `list??`，将显示实现该函数的 Python 代码。



## 三、线性神经网络

### 3-1：线性回归

presentation:突然多了很多笔记，但愿笔记会越来越少，学会筛重要信息记录。该小结主要以线性回归切入讲了一些DL中的关键要素：回归，损失函数，正态分布，最小化目标函数，最大似然估计，梯度下降。



### 3-2: 线性回归的从零开始实现

深度网络是如何实现和优化的。表面知识。



### 3-3： 线性回归的简洁实现

学习使用 PyTorch 的高级 API更简洁地实现模型。

 PyTorch 中，`data` 模块提供了数据处理工具，`nn` 模块定义了大量的神经网络层和常见损失函数。

通过`_` 结尾的方法将参数替换，从而初始化参数。



### 3-4、softmax回归

- softmax运算获取一个向量并将其映射为概率。
- softmax回归适用于分类问题。它使用了softmax运算中输出类别的概率分布。
- 交叉熵是一个衡量两个概率分布之间差异的很好的度量。它测量给定模型编码数据所需的比特数



### 3-5、图像分类数据集

数据迭代器



### 3-6、softmax回归的从零开始实现

### 3-7、softmax回归的简洁实现



## 四、多层感知机

### 4-1、多层感知机

如何结合非线性函数来构建具有更强表达能力的多层神经网络结构。

多层感知机在输出层和输入层之间增加一个或多个全连接的隐藏层，并通过激活函数转换隐藏层的输出。

常用的激活函数包括ReLU函数、sigmoid函数和tanh函数。

### 4-2、多层感知机的从零开始实现

手动实现一个简单的多层感知机。



### 4-3、多层感知机的简洁实现

使用高级API更简洁地实现多层感知机

### 4-4、模型选择、欠拟合和过拟合

欠拟合、过拟合相关概念，使用多项式回归模拟过拟合、欠拟合情况。

### 4-5、权重衰减

正则化是处理过拟合。

使用L2惩罚的权重衰减。

使用API简洁实现。

### 4-6、Dropout

dropout避免过拟合和控制权重向量的维数和大小；在训练中使用。



### 4-7、正向传播、反向传播和计算图

相关概念。



### 4-8、数值稳定性和模型初始化

梯度消失和爆炸的问题。

ReLU解决梯度消失；初始化方法；打破对称性方法。



### 4-9、环境和分布偏移

数据集分布的概念性问题。



### 4-10、实战Kaggel比赛



## 五、深度学习计算

### 5-1、层和块

由`Sequential`块处理进行层和块的顺序连接。

### 5-2、参数管理

访问、初始化和绑定模型参数的几种方法。自定义初始化方法。

### 5-3、延后初始化

通过框架自动推断参数形状。

### 5-4、自定义层

通过基本层类设计自定义层。

### 5-5、读写文件

对于张量文件的读写。

### 5-6、GPU

使用GPU进行计算。



## 六、卷积神经网络

### 6-1、从全连接层到卷积层

图像的平移不变性；

多个输入和输出通道使模型在每个空间位置可以获取图像的多方面特征。

### 6-2、图像卷积

卷积运算；如何从数据中学习卷积核的参数。

### 6-3、填充和步幅

填充和步幅可用于有效地调整数据的维度。

### 6-4、多输入通道多输出通道

多输入多输出通道可以用来扩展卷积层的模型。

当以每像素为基础应用时，1×11×1 卷积层相当于全连接层。

1×1 卷积层通常用于调整网络层的通道数量和控制模型复杂性。

### 6-5、池化层

最大池化层、平均池化层

### 6-6、卷积神经网络（LeNet)

构造最简单的神经网络LeNet



## 七、深度卷积神经网络

### 7-1、深度卷积神经网络（AlexNet）

Dropout、ReLU和预处理；Fashion-MNIST上训练；

### 7-2、使用块的网络（VGG)

VGG-11 使用可复用的卷积块构造网络。

块的使用。

特别是他们发现深层且窄的卷积（即3×33×3）比较浅层且宽的卷积更有效。

### 7-3、网络中的网络（NiN)

NiN使用由一个卷积层和多个 1×11×1 卷积层组成的块。该块可以在卷积神经网络中使用，以允许更多的每像素非线性。

NiN去除了容易造成过拟合的全连接层，将它们替换为全局平均池化层（即在所有位置上进行求和）。该池化层通道数量为所需的输出数量。

移除全连接层可减少过拟合，同时显著减少NiN的参数。

NiN的设计影响了许多后续卷积神经网络的设计。

### 7-4、含并行连接的网络（GoogLeNet)

Inception 块相当于一个有4条路径的子网络。它通过不同窗口形状的卷积层和最大池化层来并行抽取信息，并使用 1×1 卷积层减少每像素级别上的通道维数从而降低模型复杂度。

GoogLeNet将多个设计精细的Inception块与其他层（卷积层、全连接层）串联起来。其中Inception块的通道数分配之比是在 ImageNet 数据集上通过大量的实验得来的。

GoogLeNet 和它的后继者们一度是 ImageNet 上最有效的模型之一：它以较低的计算复杂度提供了类似的测试精度。

### 7-5、批量归一化

在模型训练过程中，批量归一化利用小批量的均值和标准差，不断调整神经网络的中间输出，使整个神经网络各层的中间输出值更加稳定。

批量归一化在全连接层和卷积层的使用略有不同

批量归一化层和 dropout 层一样，在训练模式和预测模式下计算不同。

批量归一化有许多有益的副作用，主要是正则化。另一方面，”减少内部协变量偏移“的原始动机似乎不是一个有效的解释。

### 7-6、残差网络（ResNet)

嵌套函数；

残差映射可以更容易地学习同一函数，例如将权重层中的参数近似为零。

利用残差块（residual blocks）可以训练出一个有效的深层神经网络：输入可以通过层间的残余连接更快地向前传播。

### 7-7、稠密连接网络（DenseNet)

在跨层连接上，不同于 ResNet 中将输入与输出相加，稠密连接网络（DenseNet）在通道维上连结输入与输出。

DenseNet 的主要构建模块是稠密块和过渡层。

在构建 DenseNet 时，我们需要通过添加过渡层来控制网络的维数，从而再次减少信道的数量。



## 八、循环神经网络

### 8-1、序列模型

序列模型的估计需要专门的统计工具。两种流行的选择是：自回归模型和隐变量自回归模型。

对于因果模型（例如，时间是向前推进的），正向估计通常比反向估计更容易。

对于直到时间步 tt 的观测序列，其在时间步 t+kt+k 的预测输出是“kk步预测”。随着我们在预测时间上进一步增加 kk，会造成误差累积，导致预测质量下降。

### 8-2、文本预处理

文本数据预处理：文本是序列数据的一种重要形式。将文本拆分为标记，构建词汇表将标记字符串映射为数字索引，并将文本数据转换为标记索引以供模型操作。

### 8-3、语言模型和数据集

齐普夫定律；

读取长序列的方式：随机采样、顺序分区

序列数据迭代器

### 8-4、循环神经网络

循环神经网络概念；

使用循环神经网络创建字符级语言模型；

使用困惑度来评价语言模型的质量；

### 8-5、循环神经网络的从0开始实现

训练一个基于循环神经网络的字符级语言模型，根据用户提供的文本前缀生成文本；

简单的循环神经网络实现；

度裁剪可以防止渐变爆炸，但不能应对梯度消失；

### 8-6、RNN的简洁实现

深度学习框架的高级API提供了循环神经网络层的实现。

### 8-7、通过时间反向传播

截断方法：随机、规则、完整；

高效计算：在通过时间反向传播期间缓存中间值；

矩阵的高次方可能导致特征值发散或消失。这以梯度爆炸或梯度消失的形式表现出来；



## 九、现代循环神经网络

### 9-1、GRU

- 门控循环神经网络可以更好地捕获具有长时间步距离序列上的依赖关系。
- 重置门有助于捕获序列中的短期相互依赖关系。
- 更新门有助于捕获序列中的长期相互依赖关系。
- 重置门打开时，门控循环单元包含基本循环神经网络；更新门打开时，门控循环单元可以跳过子序列。

### 9-2、LSTM

- 长短期记忆网络有三种类型的门：输入门、遗忘门和控制信息流的输出门。
- 长短期记忆网络的隐藏层输出包括“隐藏状态”和“记忆单元”。只有隐藏状态会传递到输出层，记忆单元的信息完全储存在内部。

- 长短期记忆网络可以缓解梯度消失和梯度爆炸。

### 9-3、深层RNN

在深层循环神经网络中，隐藏状态信息被传递到当前层的下一时间步和下一层的当前时间步。

调用API实现2个隐藏层的RNN。

### 9-4、双向RNN

双向循环神经网络主要用于序列编码和给定双向上下文的观测估计。

双向RNN的一个错误应用：应用于语言模型，推理只有过去的数据。

### 9-5、机器翻译与数据集

机器翻译数据集预处理

### 9-6、编码器-解码器结构

- “编码器－解码器”结构可以处理长度可变的序列作为输入和输出，因此适用于机器翻译等序列转换问题。
- 编码器将长度可变的序列作为输入，并将其转换为具有形状固定的状态。
- 解码器将形状固定的编码状态映射为长度可变的序列。

### 9-7、seq2seq

- 根据“编码器-解码器”结构的设计，我们可以使用两个循环神经网络来设计一个序列到序列学习的模型。
- 在“编码器－解码器”训练中，教师强制方法将原始输出序列（而非预测结果）输入解码器。
- BLEU 是一种常用的评估自然语言处理模型的方法，它通过预测序列和标签序列之间 nn 元语法的匹配度来实现。

### 9-8、束搜索

